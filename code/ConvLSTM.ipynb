{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set script-wide constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'capas', 'num_filtros', 'tam_filtros', 'num_frames'\n",
    "EXP_FILE = './experiments/capas/2.pkl'\n",
    "\n",
    "FOLDER = './datasets/train/'\n",
    "\n",
    "CLASSES = ['n', 's', 'e', 'w', 'cw', 'aw', 't']\n",
    "\n",
    "WINDOW = 5\n",
    "ELECTRODES = 24\n",
    "TACTILE_IMAGE_ROWS = 12\n",
    "TACTILE_IMAGE_COLS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-\n",
    "#glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train dataset and split it in train - validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 12, 11)\n",
      "(0,)\n",
      "./datasets/train/n-5-ff\n",
      "(880, 5, 12, 11)\n",
      "(880, 1)\n",
      "(880, 5, 12, 11)\n",
      "(880,)\n",
      "./datasets/train/s-5-ff\n",
      "(888, 5, 12, 11)\n",
      "(888, 1)\n",
      "(1768, 5, 12, 11)\n",
      "(1768,)\n",
      "./datasets/train/e-5-ff\n",
      "(885, 5, 12, 11)\n",
      "(885, 1)\n",
      "(2653, 5, 12, 11)\n",
      "(2653,)\n",
      "./datasets/train/w-5-ff\n",
      "(828, 5, 12, 11)\n",
      "(828, 1)\n",
      "(3481, 5, 12, 11)\n",
      "(3481,)\n",
      "./datasets/train/cw-5-ff\n",
      "(885, 5, 12, 11)\n",
      "(885, 1)\n",
      "(4366, 5, 12, 11)\n",
      "(4366,)\n",
      "./datasets/train/aw-5-ff\n",
      "(883, 5, 12, 11)\n",
      "(883, 1)\n",
      "(5249, 5, 12, 11)\n",
      "(5249,)\n",
      "./datasets/train/t-5-ff\n",
      "(883, 5, 12, 11)\n",
      "(883, 1)\n",
      "(6132, 5, 12, 11)\n",
      "(6132,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.zeros((0, WINDOW, TACTILE_IMAGE_ROWS, TACTILE_IMAGE_COLS))\n",
    "labels = np.zeros((0))\n",
    "    \n",
    "print(dataset.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    file = FOLDER + class_name + '-' + str(WINDOW) + '-ff'\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    class_data = np.load(file + '-data-image.npy')\n",
    "    class_labels = np.load(file + '-labels-image.npy')\n",
    "    \n",
    "    print(class_data.shape)\n",
    "    print(class_labels.shape)\n",
    "    \n",
    "    dataset = np.append(dataset, class_data, axis=0)\n",
    "    labels = np.append(labels, np.reshape(class_labels, (class_labels.shape[0])), axis=0)\n",
    "    \n",
    "    print(dataset.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1674277 , 0.16894977, 0.168379  , 0.15753425, 0.168379  ,\n",
       "        0.16799848, 0.16799848]),\n",
       " array([0.        , 0.85714286, 1.71428571, 2.57142857, 3.42857143,\n",
       "        4.28571429, 5.14285714, 6.        ]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels, bins=len(CLASSES), density=True, facecolor='g', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reshaped\n",
      "(6132, 5, 1, 12, 11)\n"
     ]
    }
   ],
   "source": [
    "# Reshape data to include channels: batch, time, channels, height (rows), width (cols) \n",
    "CHANNELS = 1\n",
    "\n",
    "dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], CHANNELS, dataset.shape[2], dataset.shape[3]))\n",
    "\n",
    "print('Dataset reshaped')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ConvLSTM and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/automan000/Convolution_LSTM_PyTorch\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.bias = bias\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding,  bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        \n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "        self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "        self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "        \n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).to(device),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_sizes, num_classes, sequence_length,\n",
    "                 dropout=0.25, bias=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        assert len(hidden_channels) == len(kernel_sizes)\n",
    "        \n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.bias = bias\n",
    "        self._all_layers = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_sizes[i], self.bias)\n",
    "            \n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "            \n",
    "            print(name)\n",
    "            print(cell)\n",
    "        \n",
    "        # https://discuss.pytorch.org/t/global-average-pooling-in-pytorch/6721\n",
    "        self.avg_pool2d = nn.AvgPool2d((TACTILE_IMAGE_ROWS, TACTILE_IMAGE_COLS))\n",
    "        self.fc1 = nn.Linear(hidden_channels[-1], hidden_channels[-1])\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_channels[-1])\n",
    "        self.fc1_drop = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        \n",
    "        for step in range(self.sequence_length):\n",
    "            #print('#### step ', step)\n",
    "            x = input[:, step, :, :, :]\n",
    "            \n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                \n",
    "                if step == 0:\n",
    "                    #print('init hidden')\n",
    "                    #print('input', x.shape)\n",
    "                    #print(name)\n",
    "                    \n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i], \n",
    "                                                             shape=(height, width))\n",
    "                    #print('h', h.shape)\n",
    "                    #print('c', c.shape)\n",
    "                    \n",
    "                    internal_state.append((h, c))\n",
    "                    \n",
    "                    #print('internal_state', len(internal_state))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "                \n",
    "            # only record last steps\n",
    "            #if step == self.sequence_length - 1:\n",
    "            #    outputs.append(x)\n",
    "            #    print('output', x.shape)\n",
    "                \n",
    "        #return outputs, (x, new_c)\n",
    "        \n",
    "        # Get last step and pass it to classifier\n",
    "        out = self.avg_pool2d(x)\n",
    "        out = torch.squeeze(out)\n",
    "        #out = F.relu(self.fc1_bn(self.fc1(out)))\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc1_drop(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 Batch: 32 Steps: 192\n",
      "cell0\n",
      "ConvLSTMCell(\n",
      "  (Wxi): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whi): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxf): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whf): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxc): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whc): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxo): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Who): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      ")\n",
      "cell1\n",
      "ConvLSTMCell(\n",
      "  (Wxi): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whi): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxf): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whf): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxc): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Whc): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Wxo): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Who): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      ")\n",
      "===== ( 1 / 5 ) - epoch 1 =====\n",
      ">> lr: 0.001\n",
      "Elapsed: 1.8141465187072754\n",
      "Elapsed: 0.03789854049682617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brayan\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:102: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "C:\\Users\\Brayan\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:103: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.03092050552368164\n",
      "Elapsed: 0.028925180435180664\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.037897348403930664\n",
      "Elapsed: 0.022937774658203125\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.022937774658203125\n",
      "Elapsed: 0.03092193603515625\n",
      "Epoch [1/30], Step [10/192], Loss: 1.939571\n",
      "Elapsed: 0.02992081642150879\n",
      "Elapsed: 0.029906749725341797\n",
      "Elapsed: 0.030913829803466797\n",
      "Elapsed: 0.02792501449584961\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.027924299240112305\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.024933338165283203\n",
      "Elapsed: 0.03390932083129883\n",
      "Elapsed: 0.028924226760864258\n",
      "Epoch [1/30], Step [20/192], Loss: 1.954653\n",
      "Elapsed: 0.03291034698486328\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.027925491333007812\n",
      "Elapsed: 0.02293872833251953\n",
      "Elapsed: 0.022939443588256836\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022937774658203125\n",
      "Elapsed: 0.022938013076782227\n",
      "Epoch [1/30], Step [30/192], Loss: 1.967641\n",
      "Elapsed: 0.028923749923706055\n",
      "Elapsed: 0.037905216217041016\n",
      "Elapsed: 0.02792072296142578\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.022937536239624023\n",
      "Elapsed: 0.025925874710083008\n",
      "Elapsed: 0.0229341983795166\n",
      "Elapsed: 0.023936986923217773\n",
      "Elapsed: 0.02294754981994629\n",
      "Elapsed: 0.023935794830322266\n",
      "Epoch [1/30], Step [40/192], Loss: 1.945224\n",
      "Elapsed: 0.0349125862121582\n",
      "Elapsed: 0.03789973258972168\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.022934675216674805\n",
      "Elapsed: 0.022945404052734375\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.029921293258666992\n",
      "Elapsed: 0.02293848991394043\n",
      "Epoch [1/30], Step [50/192], Loss: 1.947758\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.02292943000793457\n",
      "Elapsed: 0.021941184997558594\n",
      "Elapsed: 0.023936748504638672\n",
      "Elapsed: 0.03590130805969238\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022933244705200195\n",
      "Elapsed: 0.03291201591491699\n",
      "Epoch [1/30], Step [60/192], Loss: 1.952287\n",
      "Elapsed: 0.022938013076782227\n",
      "Elapsed: 0.022939443588256836\n",
      "Elapsed: 0.029921531677246094\n",
      "Elapsed: 0.03291153907775879\n",
      "Elapsed: 0.02593207359313965\n",
      "Elapsed: 0.037901878356933594\n",
      "Elapsed: 0.029920339584350586\n",
      "Elapsed: 0.029924631118774414\n",
      "Elapsed: 0.0279238224029541\n",
      "Elapsed: 0.026927947998046875\n",
      "Epoch [1/30], Step [70/192], Loss: 1.946595\n",
      "Elapsed: 0.0319058895111084\n",
      "Elapsed: 0.022938251495361328\n",
      "Elapsed: 0.022935152053833008\n",
      "Elapsed: 0.022937297821044922\n",
      "Elapsed: 0.022938966751098633\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.022938013076782227\n",
      "Elapsed: 0.02692866325378418\n",
      "Elapsed: 0.02293848991394043\n",
      "Elapsed: 0.02293682098388672\n",
      "Epoch [1/30], Step [80/192], Loss: 1.951712\n",
      "Elapsed: 0.026927709579467773\n",
      "Elapsed: 0.02293872833251953\n",
      "Elapsed: 0.021942853927612305\n",
      "Elapsed: 0.02293539047241211\n",
      "Elapsed: 0.021941184997558594\n",
      "Elapsed: 0.022937774658203125\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=FOLDS, shuffle=True)\n",
    "\n",
    "cv_train_indices = []\n",
    "cv_val_indices = []\n",
    "\n",
    "cv_loss_history = []\n",
    "cv_loss_history_val = []\n",
    "cv_accuracy_history = []\n",
    "cv_accuracy_history_val = []\n",
    "\n",
    "cv_precision = []\n",
    "cv_recall = []\n",
    "cv_f1_score = []\n",
    "cv_conf_matrix = []\n",
    "\n",
    "cv_models_dict = {}\n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(kfold.split(dataset, labels)):\n",
    "    cv_train_indices.append(train_indices)\n",
    "    cv_val_indices.append(val_indices)\n",
    "        \n",
    "    # split data\n",
    "    data_train = np.copy(dataset)\n",
    "    labels_train = np.copy(labels)\n",
    "    \n",
    "    data_val = np.copy(dataset)\n",
    "    labels_val = np.copy(labels)\n",
    "    \n",
    "    # Changed for using GPU\n",
    "    data_val = torch.from_numpy(data_val)\n",
    "    data_val = data_val.to(device, dtype=torch.float)\n",
    "\n",
    "    labels_val = torch.from_numpy(labels_val)\n",
    "    labels_val = labels_val.to(device, dtype=torch.long)\n",
    "    \n",
    "    # Train parameters\n",
    "    epochs = 30\n",
    "    batch = 32\n",
    "    \n",
    "    loss_history = []\n",
    "    loss_history_val = []\n",
    "    acc_history = []\n",
    "    acc_history_val = []\n",
    "\n",
    "    steps = math.ceil(data_train.shape[0] / batch)\n",
    "    print('Epochs:', epochs, 'Batch:', batch, 'Steps:', steps)\n",
    "\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "    # Create ConvLSTM\n",
    "    hidden_channels = [16, 16]\n",
    "    kernel_sizes = [3, 3]\n",
    "    dropout = 0.0\n",
    "\n",
    "    convlstm = ConvLSTM(input_channels=CHANNELS, hidden_channels=hidden_channels, kernel_sizes=kernel_sizes,\n",
    "                        num_classes=len(CLASSES), sequence_length=WINDOW, dropout=dropout).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    learning_rate = 0.001\n",
    "    l2_reg = 0.00\n",
    "    step_size = 60\n",
    "    lr_decay = 0.5\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(convlstm.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=lr_decay)\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        print('===== (', str(index + 1), '/', FOLDS, ') - epoch', epoch + 1, '=====')\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_val = 0.0\n",
    "        epoch_correct = 0\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('>> lr:', param_group['lr'])\n",
    "\n",
    "        # Shuffle dataset so epochs receive data in batches with different order\n",
    "        data_train, labels_train = shuffle(data_train, labels_train)\n",
    "\n",
    "        for i in range(steps):\n",
    "            # get the inputs\n",
    "            #batch, time, channels, height, width\n",
    "            batch_data = torch.from_numpy(data_train[batch * i : batch * i + batch])\n",
    "            batch_labels = torch.from_numpy(labels_train[batch * i : batch * i + batch])\n",
    "\n",
    "            # Changed for using GPU\n",
    "            batch_data = batch_data.to(device, dtype=torch.float)\n",
    "            batch_labels = batch_labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            import time\n",
    "            \n",
    "            start = time.time()\n",
    "            outputs = convlstm(batch_data)\n",
    "            end = time.time()\n",
    "            print(\"Elapsed:\", end-start)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            epoch_loss += loss.data[0]\n",
    "            epoch_correct += (outputs.max(1)[1] == batch_labels).sum().data[0]\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.6f}'.format(epoch + 1, epochs, i + 1, steps, loss.item()))\n",
    "\n",
    "        loss_history.append(epoch_loss / steps)\n",
    "        acc_history.append(epoch_correct.item() / (steps * batch) * 100)\n",
    "\n",
    "        with torch.no_grad(): # We don't want to track gradients now because we are not training the network.\n",
    "            outputs_val = convlstm(data_val)\n",
    "            loss = criterion(outputs_val, labels_val)\n",
    "\n",
    "            epoch_loss_val = loss.data[0]\n",
    "\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            total = labels_val.size(0)\n",
    "            epoch_correct_val = (predicted_val == labels_val).sum().item()\n",
    "\n",
    "        loss_history_val.append(epoch_loss_val)\n",
    "        acc_history_val.append((epoch_correct_val / total) * 100)\n",
    "\n",
    "        liveloss.update({\n",
    "            'log loss': loss_history[-1],\n",
    "            'val_log loss': loss_history_val[-1],\n",
    "            'accuracy': acc_history[-1],\n",
    "            'val_accuracy': acc_history_val[-1]\n",
    "        })\n",
    "        liveloss.draw()\n",
    "    \n",
    "    cv_models_dict['fold-' + str(index)] = convlstm.state_dict()\n",
    "    \n",
    "    cv_loss_history.append(loss_history)\n",
    "    cv_loss_history_val.append(loss_history_val)\n",
    "    cv_accuracy_history.append(acc_history)\n",
    "    cv_accuracy_history_val.append(acc_history_val)\n",
    "        \n",
    "    # Evaluate fold on validation set\n",
    "    with torch.no_grad(): # We don't want to track gradients now because we are not training the network.\n",
    "        outputs_val = convlstm(data_val)\n",
    "\n",
    "    _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "\n",
    "    total = labels_val.size(0)\n",
    "    correct = (predicted_val == labels_val).sum().item()\n",
    "    score = correct / total\n",
    "\n",
    "    [precision, recall, f1_score, _] = precision_recall_fscore_support(labels_val, predicted_val, average=None,\n",
    "                                                                       pos_label=1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(labels_val, predicted_val)\n",
    "    \n",
    "    cv_precision.append(precision * 100)\n",
    "    cv_recall.append(recall * 100)\n",
    "    cv_f1_score.append(f1_score * 100)\n",
    "    cv_conf_matrix.append(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@ Final cross-validation score @\n",
      "87.77994791666666 +/- 1.692282598705135\n",
      "[98.04212673 99.04306397 99.684583   97.27848657 76.11459531 79.8089522\n",
      " 82.77751907] +/- [1.42906889 0.49555062 0.33705914 1.0387807  8.00950754 8.8358513\n",
      " 5.90533457]\n",
      "[99.13636364 97.20720721 99.54751131 99.51690821 83.57466063 67.3015873\n",
      " 84.71655329] +/- [ 1.06017307  2.1442577   0.70099246  0.62987463  4.36973834  7.38463496\n",
      " 10.16682541]\n",
      "[98.57871743 98.10177099 99.61416582 98.37813463 79.38259478 72.74070067\n",
      " 83.49716   ] +/- [0.92273974 1.02969441 0.34985579 0.26873461 5.20224555 6.81518499\n",
      " 6.83543214]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n@ Final cross-validation score @\")\n",
    "print(np.mean(np.array(cv_accuracy_history)[:, -1]), \"+/-\", np.std(np.array(cv_accuracy_history)[:, -1]))\n",
    "print(np.mean(np.array(cv_precision), axis=0), \"+/-\", np.std(np.array(cv_precision), axis=0))\n",
    "print(np.mean(np.array(cv_recall), axis=0), \"+/-\", np.std(np.array(cv_recall), axis=0))\n",
    "print(np.mean(np.array(cv_f1_score), axis=0), \"+/-\", np.std(np.array(cv_f1_score), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXP_FILE, 'wb+') as f:\n",
    "    # Train set information\n",
    "    pickle.dump(FOLDER, f)\n",
    "    pickle.dump(CLASSES, f)\n",
    "\n",
    "    pickle.dump(WINDOW, f)\n",
    "    pickle.dump(ELECTRODES, f)\n",
    "    pickle.dump(TACTILE_IMAGE_ROWS, f)\n",
    "    pickle.dump(TACTILE_IMAGE_COLS, f)\n",
    "\n",
    "    # Model information\n",
    "    pickle.dump(hidden_channels, f)\n",
    "    pickle.dump(kernel_sizes, f)\n",
    "    pickle.dump(dropout, f)\n",
    "    pickle.dump(learning_rate, f)\n",
    "    pickle.dump(l2_reg, f)\n",
    "    pickle.dump(step_size, f)\n",
    "    pickle.dump(lr_decay, f)\n",
    "\n",
    "    # Training information\n",
    "    pickle.dump(FOLDS, f)\n",
    "    pickle.dump(cv_train_indices, f)\n",
    "    pickle.dump(cv_val_indices, f)\n",
    "\n",
    "    pickle.dump(cv_loss_history, f)\n",
    "    pickle.dump(cv_loss_history_val, f)\n",
    "    pickle.dump(cv_accuracy_history, f)\n",
    "    pickle.dump(cv_accuracy_history_val, f)\n",
    "\n",
    "    pickle.dump(cv_precision, f)\n",
    "    pickle.dump(cv_recall, f)\n",
    "    pickle.dump(cv_f1_score, f)\n",
    "    pickle.dump(cv_conf_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cv_models_dict, EXP_FILE + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
